<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>A blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="A blog">
<meta property="og:url" content="https://bynotforget.github.io.git/index.html">
<meta property="og:site_name" content="A blog">
<meta property="og:locale">
<meta property="article:author" content="BY">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="A blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">A blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Suche"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://bynotforget.github.io.git"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-FPN-R-CNN" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/09/14/FPN-R-CNN/" class="article-date">
  <time class="dt-published" datetime="2022-09-14T01:50:27.000Z" itemprop="datePublished">2022-09-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/09/14/FPN-R-CNN/">FPN &amp; R-CNN</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>FPN</p>
<p>一、FPN提出原因</p>
<p>卷积网络中，深层网络容易响应语义特征，浅层网络容易响应图像特征。然而，在目标检测中往往因为卷积网络的这个特征带来了不少麻烦：</p>
<p>高层网络虽然能响应语义特征，但是由于Feature Map的尺寸太小，拥有的几何信息并不多，不利于目标的检测；浅层网络虽然包含比较多的几何信息，但是图像的语义特征并不多，不利于图像的分类。这个问题在小目标检测中更为突出。</p>
<p>因此，如果我们能够合并深层和浅层特征的话，同时满足目标检测和图像分类的需要，那我们的问题可能就迎刃而解.</p>
<p>二、FPN的参考思想</p>
<p>FPN使用的是图像金字塔的思想。</p>
<p>传统的图像金字塔采用输入多尺度图像的方式构建多尺度的特征。简单来说，就是我们输入一张图像后，我们可以通过一些手段获得多张不同尺度的图像，我们将这些不同尺度的图像的4个顶点连接起来，就可以构造出一个类似真实金字塔的一个图像金字塔。整个过程有点像是我们看一个物品由远及近的过程（近大远小原理）。</p>
<p>其中，中间的图像是原始图像，尺寸越来越小的图片是经过下采样处理后的结果，而尺寸越来越大的图片是经过上采样处理后的结果。这样我们可以提取到更多的有用的信息。</p>
<p>三、特征金字塔</p>
<p>运用这种金字塔的思想可以提高算法的性能，但是需要大量的运算和内存。</p>
<p>因此特征金字塔要在速度和准确率之间进行权衡，通过它获得更加鲁棒的语义信息。</p>
<p>图像中存在不同大小的目标，而不同的目标具有不同的特征，所以我们需要特征金字塔来利用浅层的特征将简单的目标区分开，利用深层的特征将复杂的目标区分开。即利用大的特征图区分简单目标，利用小的特征图区分复杂目标。</p>
<p>四、FPN具体思路</p>
<p>提出的思路如下图所示：</p>
<p><img src="/2022/09/14/FPN-R-CNN/1.png"></p>
<p>图（a）：</p>
<p>先对原始图像构造图像金字塔，然后在图像金字塔的每一层提出不同的特征，然后进行相应的预测。优点：精度不错；缺点：计算量大得恐怖，占用内存大。直接pass！</p>
<p>图（b）：</p>
<p>通过对原始图像进行卷积和池化操作来获得不同尺寸的feature map，在图像的特征空间中构造出金字塔。</p>
<p>因为浅层的网络更关注于细节信息，高层的网络更关注于语义信息，更有利于准确检测出目标，因此利用最后一个卷积层上的feature map来进行预测分类。</p>
<p>优点：速度快、内存少。缺点：仅关注深层网络中最后一层的特征，却忽略了其它层的特征。</p>
<p>图（c）：</p>
<p>同时利用低层特征和高层特征。就是首先在原始图像上面进行深度卷积，然后分别在不同的特征层上面进行预测。</p>
<p>优点：在不同的层上面输出对应的目标，不需要经过所有的层才输出对应的目标（即对于有些目标来说，不用进行多余的前向操作），速度更快，又提高了算法的检测性能。</p>
<p>缺点：获得的特征不鲁棒，都是一些弱特征（因为很多的特征都是从较浅的层获得的）。</p>
<p>图（d）这才是我们真正的FPN</p>
<p>简单概括来说就是：自下而上，自上而下，横向连接和卷积融合。</p>
<p>整体过程：</p>
<p>（1）自下而上：先把预处理好的图片送进预训练的网络，比如像ResNet这些，这一步就是构建自下而上的网络，就是对应下图中的（1，2，3）这一组金字塔。</p>
<p>（2）自上而下：将层3进行一个复制变成层4，对层4进行上采样操作（就是2 * up），再用1 * 1卷积对层2进行降维处理，然后将两者对应元素相加（这里就是高低层特征的一个汇总），这样我们就得到了层5，层6以此类推，是由层5和层1进行上述操作得来的。这样就构成了自上而下网络，对应下图（4，5，6）金字塔。（其中的层2与上采样后的层4进行相加，就是横向连接的操作）</p>
<p>（3）卷积融合：最后我们对层4，5，6分别来一个3 * 3卷积操作得到最终的预测（对应下图的predict）。</p>
<p><img src="/2022/09/14/FPN-R-CNN/2.png"></p>
<p>R-CNN</p>
<p>一：初识R-CNN<br>R-CNN系列论文（R-CNN,fast-RCNN,faster-RCNN）是使用深度学习进行物体检测的鼻祖论文，其中fast-RCNN 以及faster-RCNN都是沿袭R-CNN的思路。</p>
<p>R-CNN全称region with CNN features，其实它的名字就是一个很好的解释。用CNN提取出Region Proposals中的featues，然后进行SVM分类与bbox的回归。</p>
<p>[网络结构]<br><img src="/2022/09/14/FPN-R-CNN/3.png"></p>
<p>二：训练步骤<br>1.RP的确定<br>首先介绍一下Selective Search算法，训练过程中用于从输入图像中搜索出2000个Region Proposal。Selective Search算法主要步骤：</p>
<p>使用一种过分割手段，将图像分割成小区域 (1k~2k 个)<br>计算所有邻近区域之间的相似性，包括颜色、纹理、尺度等<br>将相似度比较高的区域合并到一起<br>计算合并区域和临近区域的相似度<br>重复3、4过程，直到整个图片变成一个区域<br>在每次迭代中，形成更大的区域并将其添加到区域提议列表中。这种自下而上的方式可以创建从小到大的不同scale的Region Proposal，如图所示:</p>
<p><img src="/2022/09/14/FPN-R-CNN/4.png"></p>
<p>2.模型pre-training<br>在实际测试的时候，模型需要通过CNN提取出RP中的特征，用于后面的分类与回归。所以，如何训练好CNN成为重中之重。</p>
<p>由于物体标签训练数据少，如果要直接采用随机初始化CNN参数的方法是不足以从零开始训练出一个好的CNN模型。基于此，采用有监督的预训练，使用一个大的数据集（ImageNet ILSVC 2012）来训练AlexNet，得到一个1000分类的预训练（Pre-trained）模型。</p>
<p><img src="/2022/09/14/FPN-R-CNN/5.png"></p>
<p>3.Fine-Tunning<br>因为R-CNN模型实际测试时，是通过CNN对VOC测试集中每张输入图像上搜索到的2000个Region Proposal提取特征的。而RP大小都不相同，且AlexNet要求输入图像大小是227×227，所以需要对RP进行resize操作，将它们变形为227×227。变形之前，我们先在候选框周围加上16的padding,再进行各向异性缩放。 这种形变使得mAp提高了3到5个百分点。</p>
<p>而原CNN模型针对ImageNet数据集且无变形的图像来提取特征，现在却是针对VOC检测数据集且变形的图像来提取特征。所以，为了让我们的CNN适应新的任务（即检测任务）和新的领域（变形后的推荐窗口），需要对CNN做特定领域的参数调优，也就是fine-tunning。用的是从每张VOC训练图像中搜索到的Region Proposal进行微调的。</p>
<p>( 备注：还有一个原因，如果不针对特定任务进行fine-tuning，而是把CNN当做特征提取器，卷积层所学到的特征其实就是基础的共享特征提取层，就类似于SIFT算法一样，可以用于提取各种图片的特征，而f6、f7所学习到的特征是用于针对特定任务的特征。打个比方：对于人脸性别识别来说，一个CNN模型前面的卷积层所学习到的特征就类似于学习人脸共性特征，然后全连接层所学习的特征就是针对性别分类的特征了）</p>
<p>首先对 PASCAL VOC数据集 进行Selective Search，搜索到2000个Region Proposal对Pre-trained模型进行fine-tuning。将原来预训练模型最后的1000-way的全连接层（分类层）换成21-way的分类层（20类物体+背景），然后计算每个region proposal和ground truth 的IoU，对于IoU&gt;0.5的region proposal被视为正样本，否则为负样本（即背景）。另外，由于对于一张图片的多有候选区域来说，负样本是远远大于正样本数，所以需要将正样本进行上采样来保证样本分布均衡。在每次迭代的过程中，选择层次采样，每个mini-batch中采样两张图像，从中随机选取32个正样本和96个负样本组成一个mini-batch（128，正负比：1：3）。我们使用0.001的学习率和SGD来进行训练。</p>
<p><img src="/2022/09/14/FPN-R-CNN/6.png"></p>
<p>4.提取并保存RP的特征向量<br>提取特征的CNN网络经过了预训练和微调后不再训练，就固定不变了，只单纯的作为一个提特征的工具了。虽然文中训练了CNN网络对region proposal进行分类，但是实际中，这个CNN的作用只是提取每个region proposal的feature。<br>所以，我们输入VOC训练数据集，SS搜索出2000个RP后输入进CNN进行前向传播，然后保存AlexNet的FC7层4096维的features，以供后续的SVM分类使用。</p>
<p>5.SVM的训练<br>作者使用SVM进行分类。对于每一类都会训练一个SVM分类器，所以共有N（21）个分类器，我们来看一下是如何训练和使用SVM分类器的。</p>
<p><img src="/2022/09/14/FPN-R-CNN/7.png"><br>在SVM分类过程中，IOU&lt;0.3被作为负例，ground-truth（即完全框住了物体，默认IOU＞0.7时）是正例，其余的全部丢弃。然后SVM分类器也会输出一个预测的labels，然后用labels和truth labels比较，计算出loss，然后训练SVM。</p>
<p>其中，有一个细节，就是SVM由于是小样本训练，所以会存在负样本远多于正样本的情况。针对这种情况，作者使用了hard negative mining方法（初始时用所有样本训练，但是这样负样本可能远多王正样本，经过一轮训练后将score最高即最容易被误判的负样本加入新的样本训练集，进行训练，重复以上步骤至达到停止条件比如分类器性能不再提升），使得SVM适用于小样本训练，在样本不平衡时依然可以做到不会发生过拟合。</p>
<p>作者为什么要Hard Negatives？因为，负样本数目巨大，其中Pos样本数目占的比例特别低，负样本太多，直接导致优化过程很慢，因为很多负样本远离分界面对于优化几乎没有帮助（SVM分类最大间隔，只有支持向量比较有用）。Data-minig的作用就是去掉那些对优化作用很小的Easy-examples保留靠近分界面的Hard-examples。</p>
<p>为什么要专门使用SVM分类，而不是CNN最后的21层softmax层分类？<br>之前在训练CNN提取特征时，设置的IOU是0.5以上为正样本，小于0.5的是负样本。但在SVM分类中，只有bbox完全包围了物体（也可以理解为IOU＞0.7时）才是正样本，IOU小于0.3的是负样本。前者是大样本训练，后者是小样本训练。对于CNN的训练，需要大量的数据，不然容易过拟合，所以设置的阈值较低，比如一个bounding box可能只包含物体的一部分，那么我也把它标注为正样本，用于训练cnn；然而svm训练的时候，因为svm适用于少样本训练，所以对于训练样本数据的IOU要求比较严格，我们只有当bounding box把整个物体都包含进去了（IOU＞0.7），我们才把它标注为物体类别，IOU＜0.3的标注为负样本，然后训练svm。就因为这个特点，只能用低IOU来softmax分类，那么很可能导致最后的bbox框位置不准确（如果bbox和GT差距太大，通过线性回归会无法收敛），同时类识别精度也不高，根据实验MAP会下降几个百分点。如果硬要提高IOU，又会导致训练数据样本太少，发生过拟合。罪魁祸首就是Small VOC的训练量太少了，限制了太多优化操作。故最后选择了SVM完成分类，CNN只用来提取特征。</p>
<p>6.bbox regression的训练<br>与GT的IOU＞0.6的RP作为正样本，做回归训练。<br><img src="/2022/09/14/FPN-R-CNN/8.png"><br><img src="/2022/09/14/FPN-R-CNN/9.png"><br>        就是训练 d 矩阵向 t 矩阵靠齐的过程。</p>
<p>Fast R-CNN</p>
<p>Fast R-CNN 主要是在R-CNN和SPPNet的基础上进行改进的，有着以下几个优点：</p>
<p>1.与R-CNN、SPPNet相比，有着更高的准确率。<br>2.通过使用多任务损失，将模型训练由多阶段转变为单阶段训练。<br>3.训练时可以一次更新网络的所有层，不再需要分步更新参数。<br>4.不再需要硬盘来存储CNN提取的特征数据<br><img src="/2022/09/14/FPN-R-CNN/10.png"><br>            Fast R-CNN的网络结构</p>
<p>Fast R-CNN的流程主要分为三步：</p>
<p>1.使用 Selective Search 方法生成2K个图片候选区域。<br>2.对整张图片进行特征提取得到相应的特征图（这是对R-CNN的一大改进，参考了SPPNet），并将上一步生成的候选区域映射到特征图中。<br>3.使用ROI Pooling将所有的候选区域特征统一缩放到7*7大小，然后将这2K个特征向量展平，并连接到全连接层上，得到两个输出结果，一个是K+1类(类别数+背景类)的概率，还有一个是每个类的预测边框。<br><img src="/2022/09/14/FPN-R-CNN/11.png"><br>            Fast R-CNN的具体网络结构</p>
<p><img src="/2022/09/14/FPN-R-CNN/12.png"></p>
<p> 在R-CNN中为了统一输入使用了比较暴力的方法，但在Fast R-CNN中，使用了RoI Pooling，这一方法参考了SPPNet的空间金字塔池化，可以将RoI Pooling看做空间金字塔池化的一个简化版。简单来说，RoI Pooling 就是将每一个候选区域的特征图分割成7<em>7&#x3D;49等分，对于每一个区域使用最大池化操作，这样就能将所有输入统一到7</em>7的大小。<br><img src="/2022/09/14/FPN-R-CNN/13.png"></p>
<p>使用了多任务的损失函数来简化R-CNN中的多阶段训练。<br><img src="/2022/09/14/FPN-R-CNN/14.png"></p>
<p>Fast RCNN总结：<br>在原文中，作者说Fast RCNN是R-CNN和SPPNet的一个快速更新，所以改进的内容并不是特别多，与后面的Faster RCNN提出了RPN相比，这只能算是一次打补丁的行为。</p>
<p>文中作者更多关心的是一些其它的问题，此处由于篇幅所限，仅简单列举一下作者研究的问题以及相关的结论，具体细节请自行看论文。</p>
<p>1.多任务训练是否有帮助？<br>结论：是的，多任务训练效果比分阶段训练更好。<br>2. 尺度不变性:暴力方法还是巧妙技巧？</p>
<p>人话版：在统一图像特征尺度时，是使用SPPNet的空间金字塔池化还是本文中的ROI池化？<br>结论：两者效果差距微乎其微，甚至空间金字塔池化因为计算开销大，计算所需时间更长。<br>3. 训练数据是不是越多越好？</p>
<p>结论：将训练数据翻倍，可以将mAP提高2%~3%。<br>4. SVM的表现是否优于softmax？</p>
<p>结论：网络直接输出各类概率(softmax)，比SVM分类器性能略好<br>5. 候选区域是不是越多越好？</p>
<p>结论：否，候选区域从1K增加到10K的过程中，mAP先有所提升，然后略有下降，而且如果使用更多的候选区域，不仅没有帮助，反而会损害精度。</p>
<p><img src="/2022/09/14/FPN-R-CNN/15.png"><br>Fast RCNN不足：<br>候选区域的选取还是通过selective search，并且只能在CPU中运行这个算法，所以这个阶段浪费了大量时间。（这也是Faster RCNN改进的点）</p>
<p>Faster RCNN </p>
<p>Faster R-CNN算是RCNN系列算法的最杰出产物，也是two-stage中最为经典的物体检测算法。</p>
<p>第一阶段生成图片中待检测物体的anchor矩形框（对背景和待检测物体进行二分类）<br>第二阶段对anchor框内待检测物体进行分类。<br>Faster RCNN可以看作 RPN+Fast RCNN，其中RPN使用CNN来生成候选区域，并且RPN网络可以认为是一个使用了注意力机制的候选区域选择器，具体的网络结构如下图所示：</p>
<p><img src="/2022/09/14/FPN-R-CNN/16.png"></p>
<p>整个Faster RCNN网络可以分为三个部分，即backnone、RPN以及Roi pooling与分类网络，如下图所示：</p>
<p>首先缩放至固定大小MxN，然后将MxN图像送入网络；<br>而Conv layers中包含了13个conv层+13个relu层+4个pooling层；<br>RPN网络首先经过3x3卷积，再分别生成positive anchors和对应bounding box regression偏移量，然后计算出proposals；<br>而Roi Pooling层则利用proposals从feature maps中提取proposal feature送入后续全连接和softmax网络作classification（即分类proposal到底是什么object）。<br><img src="/2022/09/14/FPN-R-CNN/17.png"></p>
<p>1 Conv layers<br>Conv layers包含了conv，pooling，relu三种层。以python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构为例，如图2，Conv layers部分共有13个conv层，13个relu层，4个pooling层。这里有一个非常容易被忽略但是又无比重要的信息，在Conv layers中：</p>
<p>所有的conv层都是：kernel_size&#x3D;3，pad&#x3D;1，stride&#x3D;1<br>所有的pooling层都是：kernel_size&#x3D;2，pad&#x3D;0，stride&#x3D;2<br>为何重要？在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（ pad&#x3D;1，即填充一圈0），导致原图变为 (M+2)x(N+2)大小，再做3x3卷积后输出MxN 。正是这种设置，导致Conv layers中的conv层不改变输入和输出矩阵大小。如图：<br><img src="/2022/09/14/FPN-R-CNN/18.png"></p>
<p>类似的是，Conv layers中的pooling层kernel_size&#x3D;2，stride&#x3D;2。这样每个经过pooling层的MxN矩阵，都会变为(M&#x2F;2)x(N&#x2F;2)大小。综上所述，在整个Conv layers中，conv和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1&#x2F;2。</p>
<p>那么，一个MxN大小的矩阵经过Conv layers固定变为(M&#x2F;16)x(N&#x2F;16)！这样Conv layers生成的feature map中都可以和原图对应起来。</p>
<p>2 Region Proposal Networks(RPN)<br>经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如R-CNN使用SS(Selective Search)方法生成检测框。而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster R-CNN的巨大优势，能极大提升检测框的生成速度。</p>
<p><img src="/2022/09/14/FPN-R-CNN/19.png"></p>
<p>上图展示了RPN网络的具体结构。可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得positive和negative分类，下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。</p>
<p>2.1 多通道图像卷积基础知识介绍</p>
<p>对于多通道图像+多卷积核做卷积，计算方式如下：<br><img src="/2022/09/14/FPN-R-CNN/20.png"></p>
<p>如上图，输入有3个通道，同时有2个卷积核。对于每个卷积核，先在输入3个通道分别作卷积，再将3个通道结果加起来得到卷积输出。所以对于某个卷积层，无论输入图像有多少个通道，输出图像通道数总是等于卷积核数量！</p>
<p>对多通道图像做1x1卷积，其实就是将输入图像于每个通道乘以卷积系数后加在一起，即相当于把原图像中本来各个独立的通道“联通”在了一起。</p>
<p>2.2 anchors<br>提到RPN网络，就不能不说anchors。所谓anchors，实际上就是一组由rpn&#x2F;generate_anchors.py生成的矩形。直接运行作者demo中的generate_anchors.py可以得到以下输出：</p>
<p>[[ -84.  -40.   99.   55.]<br> [-176.  -88.  191.  103.]<br> [-360. -184.  375.  199.]<br> [ -56.  -56.   71.   71.]<br> [-120. -120.  135.  135.]<br> [-248. -248.  263.  263.]<br> [ -36.  -80.   51.   95.]<br> [ -80. -168.   95.  183.]<br> [-168. -344.  183.  359.]]</p>
<p>其中每行的4个值 (x1,y1,x2,y2)表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为w:h∈{1:1，1:2，1:2}三种，如下图。实际上通过anchors就引入了检测中常用到的多尺度方法。<br><img src="/2022/09/14/FPN-R-CNN/21.png"><br>注：关于上面的anchors size，其实是根据检测图像设置的。在python demo中，会把任意大小的输入图像reshape成800x600（即图2中的M&#x3D;800，N&#x3D;600）。再回头来看anchors的大小，anchors中长宽1:2中最大为352x704，长宽2:1中最大736x384，基本是cover了800x600的各个尺度和形状。</p>
<p>那么这9个anchors是做什么的呢？借用Faster RCNN论文中的原图，如图7，遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置。<br><img src="/2022/09/14/FPN-R-CNN/22.png"></p>
<p>在原文中使用的是ZF model中，其Conv Layers中最后的conv5层num_output&#x3D;256，对应生成256张特征图，所以相当于feature map每个点都是256-dimensions<br>在conv5之后，做了rpn_conv&#x2F;3x3卷积且num_output&#x3D;256，相当于每个点又融合了周围3x3的空间信息（猜测这样做也许更鲁棒？反正我没测试），同时256-d不变（如图4和图7中的红框）<br>假设在conv5 feature map中每个点上有k个anchor（默认k&#x3D;9），而每个anhcor要分positive和negative，所以每个点由256d feature转化为cls&#x3D;2•k scores；而每个anchor都有(x, y, w, h)对应4个偏移量，所以reg&#x3D;4•k coordinates<br>补充一点，全部anchors拿去训练太多了，训练程序会在合适的anchors中随机选取128个postive anchors+128个negative anchors进行训练（什么是合适的anchors下文5.1有解释）<br>注意，在本文讲解中使用的VGG conv5 num_output&#x3D;512，所以是512d，其他类似。</p>
<p>其实RPN最终就是在原图尺度上，设置了密密麻麻的候选Anchor。然后用cnn去判断哪些Anchor是里面有目标的positive anchor，哪些是没目标的negative anchor。所以，仅仅是个二分类而已！</p>
<p>那么Anchor一共有多少个？原图800x600，VGG下采样16倍，feature map每个点设置9个Anchor，所以：<br><img src="/2022/09/14/FPN-R-CNN/23.png"><br>其中ceil()表示向上取整，是因为VGG输出的feature map size&#x3D; 50*38。<br><img src="/2022/09/14/FPN-R-CNN/24.png"></p>
<p>2.3 softmax判定positive与negative<br>一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M&#x2F;16)x(N&#x2F;16)，不妨设 W&#x3D;M&#x2F;16，H&#x3D;N&#x2F;16。在进入reshape与softmax之前，先做了1x1卷积，如图：<br><img src="/2022/09/14/FPN-R-CNN/25.png"></p>
<p>该1x1卷积的caffe prototxt定义如下：<br>layer {<br>  name: “rpn_cls_score”<br>  type: “Convolution”<br>  bottom: “rpn&#x2F;output”<br>  top: “rpn_cls_score”<br>  convolution_param {<br>    num_output: 18   # 2(positive&#x2F;negative) * 9(anchors)<br>    kernel_size: 1 pad: 0 stride: 1<br>  }<br>}</p>
<p>可以看到其num_output&#x3D;18，也就是经过该卷积的输出图像为WxHx18大小（注意第二章开头提到的卷积计算方式）。这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是positive和negative，所有这些信息都保存WxHx(9*2)大小的矩阵。为何这样做？后面接softmax分类获得positive anchors，也就相当于初步提取了检测目标候选区域box（一般认为目标在positive anchors中）。</p>
<p>那么为何要在softmax前后都接一个reshape layer？其实只是为了便于softmax分类，至于具体原因这就要从caffe的实现形式说起了。在caffe基本数据结构blob中以如下形式保存数据：<br>    blob&#x3D;[batch_size, channel，height，width]<br>对应至上面的保存positive&#x2F;negative anchors的矩阵，其在caffe blob中的存储形式为[1, 2x9, H, W]。而在softmax分类时需要进行positive&#x2F;negative二分类，所以reshape layer会将其变为[1, 2, 9xH, W]大小，即单独“腾空”出来一个维度以便softmax分类，之后再reshape回复原状。贴一段caffe softmax_loss_layer.cpp的reshape函数的解释，非常精辟：</p>
<p>   “Number of labels must match number of predictions; “<br>    “e.g., if softmax axis &#x3D;&#x3D; 1 and prediction shape is (N, C, H, W), “<br>    “label count (number of labels) must be N<em>H</em>W, “<br>    “with integer values in {0, 1, …, C-1}.”;</p>
<p>综上所述，RPN网络中利用anchors和softmax初步提取出positive anchors作为候选区域（另外也有实现用sigmoid代替softmax，输出[1, 1, 9xH, W]后接sigmoid进行positive&#x2F;negative二分类，原理一样）。</p>
<p>2.4 bounding box regression原理<br>如图9所示绿色框为飞机的Ground Truth(GT)，红色为提取的positive anchors，即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得positive anchors和GT更加接近。<br><img src="/2022/09/14/FPN-R-CNN/26.png"><br>对于窗口一般使用四维向量(x,y,w,h)表示，分别表示窗口的中心点坐标和宽高。对于图 11，红色的框A代表原始的positive Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G’，即：<br><img src="/2022/09/14/FPN-R-CNN/27.png"></p>
<p>那么经过何种变换F才能从图中的anchor A变为G’呢？ 比较简单的思路就是:<br><img src="/2022/09/14/FPN-R-CNN/28.png"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://bynotforget.github.io.git/2022/09/14/FPN-R-CNN/" data-id="cl811lm710000agfn3vwufmwv" data-title="FPN &amp; R-CNN" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-ShuffleNet-EfficientNet" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/09/01/ShuffleNet-EfficientNet/" class="article-date">
  <time class="dt-published" datetime="2022-09-01T02:05:20.000Z" itemprop="datePublished">2022-09-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/09/01/ShuffleNet-EfficientNet/">ShuffleNet&amp;EfficientNet</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>ShuffleNet网络</p>
<p>ShuffleNet是旷视科技最近提出的一种计算高效的CNN模型，其和MobileNet和SqueezeNet等一样主要是想应用在移动端。所以，ShuffleNet的设计目标也是如何利用有限的计算资源来达到最好的模型精度，这需要很好地在速度和精度之间做平衡。ShuffleNet的核心是采用了两种操作：pointwise group convolution和channel shuffle，这在保持精度的同时大大降低了模型的计算量。目前移动端CNN模型主要设计思路主要是两个方面：模型结构设计和模型压缩。ShuffleNet和MobileNet一样属于前者，都是通过设计更高效的网络结构来实现模型变小和变快，而不是对一个训练好的大模型做压缩或者迁移。</p>
<p>ShuffleNet的核心设计理念是对不同的channels进行shuffle来解决group convolution带来的弊端。Group convolution是将输入层的不同特征图进行分组，然后采用不同的卷积核再对各个组进行卷积，这样会降低卷积的计算量。因为一般的卷积都是在所有的输入特征图上做卷积，可以说是全通道卷积，这是一种通道密集连接方式（channel dense connection）。而group convolution相比则是一种通道稀疏连接方式（channel sparse connection）。使用group convolution的网络如Xception，MobileNet，ResNeXt等。Xception和MobileNet采用了depthwise convolution，这其实是一种比较特殊的group convolution，因此此时分组数恰好等于通道数，意味着每个组只有一个特征图。但是这些网络存在一个很大的弊端是采用了密集的1x1卷积，或者说是dense pointwise convolution，这里说的密集指的是卷积是在所有通道上进行的。所以，实际上比如ResNeXt模型中1x1卷积基本上占据了93.4%的乘加运算。那么不如也对1x1卷积采用channel sparse connection，那样计算量就可以降下来了。但是group convolution存在另外一个弊端，如图1-(a)所示，其中GConv是group convolution，这里分组数是3。可以看到当堆积GConv层后一个问题是不同组之间的特征图是不通信的，这就好像分了三个互不相干的路，大家各走各的，这目测会降低网络的特征提取能力。这样你也可以理解为什么Xception，MobileNet等网络采用密集的1x1卷积，因为要保证group convolution之后不同组的特征图之间的信息交流。但是达到上面那个目的，我们不一定非要采用dense pointwise convolution。如图1-(b)所示，你可以对group convolution之后的特征图进行“重组”，这样可以保证接下了采用的group convolution其输入来自不同的组，因此信息可以在不同组之间流转。这个操作等价于图2-(c)，即group convolution之后对channels进行shuffle，但并不是随机的，其实是“均匀地打乱”。在程序上实现channel shuffle是非常容易的：假定将输入层分为g组，总通道数为g*n，首先你将通道那个维度拆分为(g,n)两个维度，然后将这两个维度转置变成(n,g)，最后重新reshape成一个维度。如果你不太理解这个操作，你可以试着动手去试一下，发现仅需要简单的维度操作和转置就可以实现均匀的shuffle。利用channel shuffle就可以充分发挥group convolution的优点，而避免其缺点。</p>
<p><img src="/2022/09/01/ShuffleNet-EfficientNet/1.jpg"><br>                                图1 使用channel shuffle后的group convolution</p>
<p>网络结构<br>基于上面的设计理念，首先来构造ShuffleNet的基本单元，如图2所示。ShuffleNet的基本单元是在一个残差单元的基础上改进而成的。如图2-（a）所示，这是一个包含3层的残差单元：首先是1x1卷积，然后是3x3的depthwise convolution（DWConv，主要是为了降低计算量），这里的3x3卷积是瓶颈层（bottleneck），紧接着是1x1卷积，最后是一个短路连接，将输入直接加到输出上。现在，进行如下的改进：将密集的1x1卷积替换成1x1的group convolution，不过在第一个1x1卷积之后增加了一个channel shuffle操作。值得注意的是3x3卷积后面没有增加channel shuffle，按paper的意思，对于这样一个残差单元，一个channel shuffle操作是足够了。还有就是3x3的depthwise convolution之后没有使用ReLU激活函数。改进之后如图2-（b）所示。对于残差单元，如果stride&#x3D;1时，此时输入与输出shape一致可以直接相加，而当stride&#x3D;2时，通道数增加，而特征图大小减小，此时输入与输出不匹配。一般情况下可以采用一个1x1卷积将输入映射成和输出一样的shape。但是在ShuffleNet中，却采用了不一样的策略，如图2-（c）所示：对原输入采用stride&#x3D;2的3x3 avg pool，这样得到和输出一样大小的特征图，然后将得到特征图与输出进行连接（concat），而不是相加。这样做的目的主要是降低计算量与参数大小。</p>
<p><img src="/2022/09/01/ShuffleNet-EfficientNet/2.jpg"><br>                                图2 ShuffleNet的基本单元</p>
<p>基于上面改进的ShuffleNet基本单元，设计的ShuffleNet模型如表1所示。可以看到开始使用的普通的3x3的卷积和max pool层。然后是三个阶段，每个阶段都是重复堆积了几个ShuffleNet的基本单元。对于每个阶段，第一个基本单元采用的是stride&#x3D;2，这样特征图width和height各降低一半，而通道数增加一倍。后面的基本单元都是stride&#x3D;1，特征图和通道数都保持不变。对于基本单元来说，其中瓶颈层，就是3x3卷积层的通道数为输出通道数的1&#x2F;4，这和残差单元的设计理念是一样的。不过有个细节是，对于stride&#x3D;2的基本单元，由于原输入会贡献一部分最终输出的通道数，那么在计算1&#x2F;4时到底使用最终的通道数，还是仅仅未concat之前的通道数。文章没有说清楚，但是个人认为应该是后者吧。其中 g 控制了group convolution中的分组数，分组越多，在相同计算资源下，可以使用更多的通道数，所以g越大时，采用了更多的卷积核。这里给个例子，当 g&#x3D;3 时，对于第一阶段的第一个基本单元，其输入通道数为24，输出通道数为240，但是其stride&#x3D;2，那么由于原输入通过avg pool可以贡献24个通道，所以相当于左支只需要产生240-24&#x3D;216通道，中间瓶颈层的通道数就为216&#x2F;4&#x3D;54。其他的可以以此类推。当完成三阶段后，采用global pool将特征图大小降为1x1，最后是输出类别预测值的全连接层。</p>
<p><img src="/2022/09/01/ShuffleNet-EfficientNet/3.jpg"><br>                                表1 ShuffleNet的网络结</p>
<p>模型效果<br>那么ShuffleNet的模型效果如何呢？表2给出了采用不同的$g$值的ShuffleNet在ImageNet上的实验结果。可以看到基本上当$g$越大时，效果越好，这是因为采用更多的分组后，在相同的计算约束下可以使用更多的通道数，或者说特征图数量增加，网络的特征提取能力增强，网络性能得到提升。注意Shuffle 1x是基准模型，而0.5x和0.25x表示的是在基准模型上将通道数缩小为原来的0.5和0.25。</p>
<p><img src="/2022/09/01/ShuffleNet-EfficientNet/4.jpg"><br>                                表2 采用不同g值的ShuffleNet的分类误差</p>
<p>除此之外，作者还对比了不采用channle shuffle和采用之后的网络性能对比，如表3所示。可以清楚的看到，采用channle shuffle之后，网络性能更好，从而证明channle shuffle的有效性。</p>
<p><img src="/2022/09/01/ShuffleNet-EfficientNet/5.jpg"><br>                                表3 不采用channle shuffle和采用之后的网络性能对比</p>
<p>然后是ShuffleNet与MobileNet的对比，如表4所示。可以看到ShuffleNet不仅计算复杂度更低，而且精度更好。</p>
<p><img src="/2022/09/01/ShuffleNet-EfficientNet/6.jpg"><br>                                表4 ShuffleNet与MobileNet对比</p>
<p>EfficientNet网络</p>
<p>卷积神经网络（ConvNets）通常是在固定的资源预算下发展起来的，如果有更多的资源可用的话，则会扩大规模以获得更好的精度，比如可以提高网络深度(depth)、网络宽度(width)和输入图像分辨率 (resolution)大小。但是通过人工去调整 depth, width, resolution 的放大或缩小的很困难的，在计算量受限时有放大哪个缩小哪个，这些都是很难去确定的，换句话说，这样的组合空间太大，人力无法穷举。基于上述背景，该论文提出了一种新的模型缩放方法，它使用一个简单而高效的复合系数来从depth, width, resolution 三个维度放大网络，不会像传统的方法那样任意缩放网络的维度，基于神经结构搜索技术可以获得最优的一组参数(复合系数)。从下图可看出，EfficientNet不仅比别的网络快很多，而且精度也更高。</p>
<p><img src="/2022/09/01/ShuffleNet-EfficientNet/7.jpg"></p>
<p>本篇论文中会同时增加网络的width、网络的深度以及输入网络的分辨率来提升网络的性能如图所示：</p>
<p><img src="/2022/09/01/ShuffleNet-EfficientNet/8.jpg"></p>
<p>根据以往的经验，增加网络的深度depth能够得到更加丰富、复杂的特征并且能够很好的应用到其它任务中。但网络的深度过深会面临梯度消失，训练困难的问题。<br>增加网络的width能够获得更高细粒度的特征并且也更容易训练，但对于width很大而深度较浅的网络往往很难学习到更深层次的特征。<br>增加输入网络的图像分辨率能够潜在得获得更高细粒度的特征模板，但对于非常高的输入分辨率，准确率的增益也会减小。并且大分辨率图像会增加计算量。<br>下图展示了在基准EfficientNetB-0上分别增加width、depth以及resolution后得到的统计结果。通过下图可以看出大概在Accuracy达到80%时就趋于饱和了。</p>
<p><img src="/2022/09/01/ShuffleNet-EfficientNet/9.jpg"></p>
<p>接着作者又做了一个实验，采用不同的d，r组合，然后不断改变网络的width就得到了如下图所示的4条曲线，通过分析可以发现在相同的FLOPs下，同时增加d和r的效果最好。</p>
<p><img src="/2022/09/01/ShuffleNet-EfficientNet/10.jpg"></p>
<p>网络详细结构<br>下表为EfficientNet-B0的网络框架（B1-B7就是在B0的基础上修改Resolution，Channels以及Layers），可以看出网络总共分成了9个Stage，第一个Stage就是一个卷积核大小为3x3步距为2的普通卷积层（包含BN和激活函数Swish），Stage2～Stage8都是在重复堆叠MBConv结构（最后一列的Layers表示该Stage重复MBConv结构多少次），而Stage9由一个普通的1x1的卷积层（包含BN和激活函数Swish）一个平均池化层和一个全连接层组成。表格中每个MBConv后会跟一个数字1或6，这里的1或6就是倍率因子n即MBConv中第一个1x1的卷积层会将输入特征矩阵的channels扩充为n倍，其中k3x3或k5x5表示MBConv中Depthwise Conv所采用的卷积核大小。Channels表示通过该Stage后输出特征矩阵的Channels。</p>
<p><img src="/2022/09/01/ShuffleNet-EfficientNet/11.jpg"></p>
<p>MBConv结构<br>MBConv其实就是MobileNetV3网络中的InvertedResidualBlock，但也有些许区别。一个是采用的激活函数不一样（EfficientNet的MBConv中使用的都是Swish激活函数），另一个是在每个MBConv中都加入了SE（Squeeze-and-Excitation）模块。下图是MBConv结构。</p>
<p><img src="/2022/09/01/ShuffleNet-EfficientNet/12.jpg"></p>
<p>如下图所示，MBConv结构主要由一个1x1的普通卷积（升维作用，包含BN和Swish），一个kxk的Depthwise Conv卷积（包含BN和Swish）k的具体值可看EfficientNet-B0的网络框架主要有3x3和5x5两种情况，一个SE模块，一个1x1的普通卷积（降维作用，包含BN），一个Droupout层构成。</p>
<p><img src="/2022/09/01/ShuffleNet-EfficientNet/13.jpg"></p>
<p>EfficientNetB0的网络结构<br><img src="/2022/09/01/ShuffleNet-EfficientNet/14.jpg"></p>
<p>原论文中关于EfficientNet与当时主流网络的性能参数对比：</p>
<p><img src="/2022/09/01/ShuffleNet-EfficientNet/15.jpg"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://bynotforget.github.io.git/2022/09/01/ShuffleNet-EfficientNet/" data-id="cl7lamw1x0000rkfn6nh53n4b" data-title="ShuffleNet&amp;EfficientNet" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-SENet-MobileNet" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/08/28/SENet-MobileNet/" class="article-date">
  <time class="dt-published" datetime="2022-08-28T07:39:35.000Z" itemprop="datePublished">2022-08-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/08/28/SENet-MobileNet/">SENet&amp;MobileNet</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>SENet网络</p>
<p>SENet网络的创新点在于关注channel之间的关系，希望模型可以自动学习到不同channel特征的重要程度。</p>
<p>对于CNN网络来说，其核心计算是卷积算子，其通过卷积核从输入特征图学习到新特征图。从本质上讲，卷积是对一个局部区域进行特征融合，这包括空间上（H和W维度）以及通道间（C维度）的特征融合。</p>
<p>对于卷积操作，很大一部分工作是提高感受野，即空间上融合更多特征融合，或者是提取多尺度空间信息，如Inception网络的多分支结构。对于channel维度的特征融合，卷积操作基本上默认对输入特征图的所有channel进行融合。而MobileNet网络中的组卷积（Group Convolution）和深度可分离卷积（Depthwise Separable Convolution）对channel进行分组也主要是为了使模型更加轻量级，减少计算量。而SENet网络的创新点在于关注channel之间的关系，希望模型可以自动学习到不同channel特征的重要程度。为此，SENet提出了Squeeze-and-Excitation (SE)模块，如下图所示：</p>
<p><img src="/2022/08/28/SENet-MobileNet/1.png"></p>
<pre><code>                       Squeeze-and-Excitation (SE)模块
</code></pre>
<p>SE模块首先对卷积得到的特征图进行Squeeze操作，得到channel级的全局特征，然后对全局特征进行Excitation操作，学习各个channel间的关系，也得到不同channel的权重，最后乘以原来的特征图得到最终特征。本质上，SE模块是在channel维度上做attention或者gating操作，这种注意力机制让模型可以更加关注信息量最大的channel特征，而抑制那些不重要的channel特征。另外一点是SE模块是通用的，这意味着其可以嵌入到现有的网络架构中。</p>
<p>SE模块<br><img src="/2022/08/28/SENet-MobileNet/2.png"></p>
<p>Squeeze操作<br>由于卷积只是在一个局部空间内进行操作，  很难获得足够的信息来提取channel之间的关系，对于网络中前面的层这更严重，因为感受野比较小。为了，SENet提出Squeeze操作，将一个channel上整个空间特征编码为一个全局特征，采用global average pooling来实现（原则上也可以采用更复杂的聚合策略）：<br><img src="/2022/08/28/SENet-MobileNet/3.png"></p>
<p>Excitation操作<br>Sequeeze操作得到了全局描述特征，我们接下来需要另外一种运算来抓取channel之间的关系。这个操作需要满足两个准则：首先要灵活，它要可以学习到各个channel之间的非线性关系；第二点是学习的关系不是互斥的，因为这里允许多channel特征，而不是one-hot形式。基于此，这里采用sigmoid形式的gating机制：<br><img src="/2022/08/28/SENet-MobileNet/4.png"></p>
<pre><code>其中![](SENet-MobileNet/5.png).
</code></pre>
<p>为了降低模型复杂度以及提升泛化能力，这里采用包含两个全连接层的bottleneck结构，其中第一个FC层起到降维的作用，降维系数为r是个超参数，然后采用ReLU激活。最后的FC层恢复原始的维度。</p>
<p>最后将学习到的各个channel的激活值（sigmoid激活，值0~1）乘以U上的原始特征：<br><img src="/2022/08/28/SENet-MobileNet/6.png"><br>整个操作可以看成学习到了各个channel的权重系数，从而使得模型对各个channel的特征更有辨别能力，这应该也算一种attention机制。</p>
<p>SE模块的灵活性在于它可以直接应用现有的网络结构中。这里以Inception和ResNet为例。对于Inception网络，没有残差结构，这里对整个Inception模块应用SE模块。对于ResNet，SE模块嵌入到残差结构中的残差学习分支中。具体如下图所示：<br><img src="/2022/08/28/SENet-MobileNet/7.png"></p>
<p>MobileNet网络</p>
<p>MobileNet V1</p>
<p>MobileNet V1是由google2016年提出，2017年发布的文章。其主要创新点在于深度可分离卷积，而整个网络实际上也是深度可分离模块的堆叠。</p>
<p>模型结构对与模型耗时的影响：<img src="/2022/08/28/SENet-MobileNet/8.png"><br>可以看到不管是在GPU还是在CPU运行，最重要的“耗时杀手”就是conv，卷积层。也就是说，想要提高网络的运行速度，就得到提高卷积层的计算效率。</p>
<p>那么什么是深度可分离卷积？<br><img src="/2022/08/28/SENet-MobileNet/9.png"><br>深度可分离卷积被证明是轻量化网络的有效设计，如上图所示，深度可分离卷积由逐深度卷积（Depthwise）和逐点卷积（Pointwise）构成。</p>
<p>对比于标准卷积，逐深度卷积将卷积核拆分成为单通道形式，在不改变输入特征图像的深度的情况下，对每一通道进行卷积操作，这样就得到了和输入特征图通道数一致的输出特征图。</p>
<p>逐点卷积就是1×1卷积。主要作用就是对特征图进行升维和降维。</p>
<p>标准卷积与深度可分离卷积详细对比：<br><img src="/2022/08/28/SENet-MobileNet/10.png"><br>那么MobileNet V1为什么会快呢？<br><img src="/2022/08/28/SENet-MobileNet/11.png"><br>对比标准卷积和深度可分离卷积的计算量就会发现，因为卷积核的尺寸K通常远小于输出通道数Cout，因此标准卷积的计算复杂度近似为 DW + PW 组合卷积的K*K倍。计算量减少，模型自然会快。</p>
<p>工程实现中一般采用如下右侧结构来实现深度可分离模块，左侧为标准卷积。其中，Relu6&#x3D;min(max(0,x),6)<br><img src="/2022/08/28/SENet-MobileNet/12.png"></p>
<pre><code>ReLU6就是普通的ReLU但是限制最大输出值为6（对输出值做clip），这是为了在移动端设备float16的低精度的时候，也能有很好的数值分辨率，如果对ReLU的激活范围不加限制，输出范围为0到正无穷，如果激活值非常大，分布在一个很大的范围内，则低精度的float16无法很好地精确描述如此大范围的数值，带来精度损失。
![](SENet-MobileNet/13.jpg)
</code></pre>
<p>MobileNet V2</p>
<p>MobileNet V1的缺点：</p>
<p>结构问题：<br>V1结构过于简单，没有复用图像特征，即没有concat&#x2F;eltwise+ 等操作进行特征融合，而后续的一系列的ResNet, DenseNet等结构已经证明复用图像特征的有效性。</p>
<p>逐深度卷积问题：<br>在处理低维数据（比如逐深度的卷积）时，relu函数会造成信息的丢失。<br>DW 卷积由于本身的计算特性决定它自己没有改变通道数的能力，上一层给它多少通道，它就只能输出多少通道。所以如果上一层给的通道数本身很少的话，DW 也只能很委屈的在低维空间提特征，因此效果不够好。</p>
<p>V2使用了跟V1类似的深度可分离结构，不同之处也正对应着V1中逐深度卷积的缺点改进：</p>
<p>V2 去掉了第二个 PW 的激活函数改为线性激活。<br>论文作者称其为 Linear Bottleneck。原因如上所述是因为作者认为激活函数在高维空间能够有效的增加非线性，而在低维空间时则会破坏特征，不如线性的效果好。<br>V2 在 DW 卷积之前新加了一个 PW 卷积。<br>给每个 DW 之前都配备了一个 PW，专门用来升维，定义升维系数t&#x3D;6，这样不管输入通道数Cin是多是少，经过第一个 PW 升维之后，DW 都是在相对的更高维 (t,Cin) 进行更好的特征提取。</p>
<p>Inverted residuals<br><img src="/2022/08/28/SENet-MobileNet/14.jpg"><br>MobileNet V2 借鉴 ResNet，都采用了 1<em>1→3</em>3→1*1 的模式。同样使用 Shortcut 将输出与输入相加（未在上式画出） .<br>但是ResNet 先降维 (0.25倍)、卷积、再升维，而 MobileNet V2 则是 先升维 (6倍)、卷积、再降维。直观的形象上来看，ResNet 的微结构是沙漏形，而 MobileNet V2 则是纺锤形，刚好相反。因此论文作者将 MobileNet V2 的结构称为 Inverted Residual Block。这么做也是因为使用DW卷积而作的适配，希望特征提取能够在高维进行。</p>
<p>MobileNet V2的深度可分离模块与MobileNet V1对比：<br><img src="/2022/08/28/SENet-MobileNet/15.jpg"></p>
<pre><code>Mobilenet v2中有两种深度可分离模块，步长为1时输入输出size相等，此时使用shortcut结构。步长为2时，由于input与outputsize不符，不添加shortcut结构。
</code></pre>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://bynotforget.github.io.git/2022/08/28/SENet-MobileNet/" data-id="cl7k7jdn70000asfndgsse3an" data-title="SENet&amp;MobileNet" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-ResNet、DenseNet" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/08/25/ResNet%E3%80%81DenseNet/" class="article-date">
  <time class="dt-published" datetime="2022-08-25T12:56:45.000Z" itemprop="datePublished">2022-08-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/08/25/ResNet%E3%80%81DenseNet/">ResNet、DenseNet</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>ResNet—什么是残差网络？</p>
<p>假设我们有如下的一个网络，它可以在训练集和测试集上可以得到很好的性能。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/1.png"><br>接着构造如下的网络，前面4层的参数复制于上面的网络，训练时这几层的参数保持不变。换言之，我们只是在上面的网络新增加了几个紫颜色表示的层。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/2.png"><br>理论上，这个新的网络在训练集或者测试集上的性能要比第一个网络的性能好，毕竟多了几个新增加的层提取特征。然后，实际上这个新的网络却比原先的网络的性能要差。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/3.png"><br>56层的网络无论是在训练集还是测试集上，误差率都比20层的要高。出现这种现象的原因并非是由于层数加深引发的梯度消失&#x2F;梯度爆炸，因为已经通过归一化的方法解决了这个问题，对于出现这种现象的原因将在下面讨论，我们将这种反常的现象称之为“退化现象“。</p>
<p>为什么会出现这样的原因呢，何凯明在文章中给出的解释是“难以对网络进行优化”。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/4.png"><br>而退化现象也表明了，实际上新增加的几个紫色的层，很难做到恒等映射。又或者能做到，但在有限的时间内很难完成（即网络要用指数级别的时间才能达到收敛）。</p>
<p>这时候，巧妙的通过添加”桥梁“，使得难以优化的问题瞬间迎刃而解。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/5.png"><br>可以看到通过添加这个桥梁，把数据原封不动得送到FC层的前面，而对于中间的紫色层，可以很容易的通过把这些层的参数逼近于0，进而实现f(x)&#x3D;x的功能。<br>通过跳连接，可以把前四层的输出先送到FC层前面，也就相当于告诉紫色层：“我已经做完前面的工作了，你看看能不能在剩下的工作中发点力，要是找不出提升性能的效果也没事的，我们可以把你的参数逼近于0。”<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/6.png"><br>神经网络无非是拟合一个复杂的函数映射关系，而通过跳链接，可以很好的“切割”这种映射关系，实现“分步”完成。我们把整个映射看成100%，则前面四层网络实现了98%的映射关系，而残余的映射由紫色层完成，Residual 另一个翻译就是”残余，残留“的意思，也就是让每一个残差块，只关注残余映射的一小部分，真的是恰到好处。当然了，实际上网络运行的时候，我们并不会知道哪几层就能达到很好的效果，然后在它们的后面接一个跳连接，于是一开始便在两个层或者三个层之间添加跳连接，形成残差块，每个残差块只关注当前的残余映射，而不会关注前面已经实现的底层映射。大概形状如下图：<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/7.png"></p>
<p>DenseNet</p>
<p>DenseNet的基本思路与ResNet一致，但是它建立的是前面所有层与后面层的密集连接（dense connection），它的名称也是由此而来。DenseNet的另一大特色是通过特征在channel上的连接来实现特征重用（feature reuse）。这些特点让DenseNet在参数和计算成本更少的情形下实现比ResNet更优的性能。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/8.png"><br>相比ResNet，DenseNet提出了一个更激进的密集连接机制：即互相连接所有的层，具体来说就是每个层都会接受其前面所有层作为其额外的输入。图1为ResNet网络的连接机制，作为对比，图2为DenseNet的密集连接机制。可以看到，ResNet是每个层与前面的某层（一般是2~3层）短路连接在一起，连接方式是通过元素级相加。而在DenseNet中，每个层都会与前面所有层在channel维度上连接（concat）在一起（这里各个层的特征图大小是相同的，后面会有说明），并作为下一层的输入。对于一个 L  层的网络，DenseNet共包含 L ( L + 1 )&#x2F; 2 个连接，相比ResNet，这是一种密集连接。而且DenseNet是直接concat来自不同层的特征图，这可以实现特征重用，提升效率，这一特点是DenseNet与ResNet最主要的区别。</p>
<p><img src="/2022/08/25/ResNet%E3%80%81DenseNet/9.png"></p>
<pre><code>ResNet网络的短路连接机制（其中+代表的是元素级相加操作）
</code></pre>
<p><img src="/2022/08/25/ResNet%E3%80%81DenseNet/10.png"></p>
<pre><code>DenseNet网络的密集连接机制（其中c代表的是channel级连接操作）
</code></pre>
<p>DenseNet的前向过程如图所示，可以更直观地理解其密集连接方式，比h3的输入不仅来自h2的x2，还包括前面两层的x1和x2，它们是在channel维度上连接在一起的。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/11.png"><br>DenseNet的密集连接方式需要特征图大小保持一致。为了解决这个问题，DenseNet网络中使用DenseBlock+Transition的结构，其中DenseBlock是包含很多层的模块，每个层的特征图大小相同，层与层之间采用密集连接方式。而Transition模块是连接两个相邻的DenseBlock，并且通过Pooling使特征图大小降低。下图给出了DenseNet的网路结构，它共包含4个DenseBlock，各个DenseBlock之间通过Transition连接在一起。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/12.png"></p>
<p>DenseNet网络结构<br>如前所示，DenseNet的网络结构主要由DenseBlock和Transition组成，如图所示。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/13.png"><br>在DenseBlock中，各个层的特征图大小一致，可以在channel维度上连接。DenseBlock中的非线性组合函数H(.)采用的是BN+ReLU+3x3 Conv的结构，如图所示。另外值得注意的一点是，与ResNet不同，所有DenseBlock中各个层卷积之后均输出k个特征图，即得到的特征图的channel数为k，或者说采用k个卷积核。k在DenseNet称为growth rate，这是一个超参数。一般情况下使用较小的 k（比如12），就可以得到较佳的性能。假定输入层的特征图的channel数为 k0 ，那么 l 层输入的channel数为 k0 + k(l-1)，因此随着层数增加，尽管k设定得较小，DenseBlock的输入会非常多，不过这是由于特征重用所造成的，每个层仅有k个特征是自己独有的。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/14.png"></p>
<pre><code>DenseBlock中的非线性转换结构
</code></pre>
<p>由于后面层的输入会非常大，DenseBlock内部可以采用bottleneck层来减少计算量，主要是原有的结构中增加1x1 Conv，如图所示，即BN+ReLU+1x1 Conv+BN+ReLU+3x3 Conv，称为DenseNet-B结构。其中1x1 Conv得到4k个特征图它起到的作用是降低特征数量，从而提升计算效率。</p>
<p><img src="/2022/08/25/ResNet%E3%80%81DenseNet/15.png"></p>
<pre><code>使用bottleneck层的DenseBlock结构 
</code></pre>
<p>对于Transition层，它主要是连接两个相邻的DenseBlock，并且降低特征图大小。Transition层包括一个1x1的卷积和2x2的AvgPooling，结构为BN+ReLU+1x1 Conv+2x2 AvgPooling。另外，Transition层可以起到压缩模型的作用。假定Transition的上接DenseBlock得到的特征图channels数为m ，Transition层可以产生θm个特征，其中θ∈(0,1] 是压缩系数（compression rate）。当θ&#x3D;1 时，特征个数经过Transition层没有变化，即无压缩，而当压缩系数小于1时，这种结构称为DenseNet-C，文中使用θ&#x3D;0.5。对于使用bottleneck层的DenseBlock结构和压缩系数小于1的Transition组合结构称为DenseNet-BC。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/" data-id="cl7jvhafh00036sfnhmoo3h1w" data-title="ResNet、DenseNet" class="article-share-link">Teilen</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-AlexNet、VGG、GoogLeNet-总结" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/08/20/AlexNet%E3%80%81VGG%E3%80%81GoogLeNet-%E6%80%BB%E7%BB%93/" class="article-date">
  <time class="dt-published" datetime="2022-08-20T09:25:29.000Z" itemprop="datePublished">2022-08-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/08/20/AlexNet%E3%80%81VGG%E3%80%81GoogLeNet-%E6%80%BB%E7%BB%93/">AlexNet、VGG、GoogLeNet 总结</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>一、AlexNet网络<br><img src="/2022/08/20/AlexNet%E3%80%81VGG%E3%80%81GoogLeNet-%E6%80%BB%E7%BB%93/1.png"><br>主要亮点有：</p>
<p>（1）首次利用 GPU 进行网络加速训练。</p>
<p>（2）使用了 ReLU 激活函数，而不是传统的 Sigmoid 激活函数以及 Tanh 激活函数。</p>
<p>（3）使用了 LRN 局部响应归一化。</p>
<p>（4）在全连接层的前两层中使用了 Dropout 随机失活神经元操作，以减少过拟合。</p>
<p>引入Dropout的作用：<br><img src="/2022/08/20/AlexNet%E3%80%81VGG%E3%80%81GoogLeNet-%E6%80%BB%E7%BB%93/2.png"><br>过拟合：根本原因是特征维度过多，模型假设过于复杂，参数 过多，训练数据过少，噪声过多，导致拟合的函数完美的预测 训练集，但对新数据的测试集预测结果差。 过度的拟合了训练 数据，而没有考虑到泛化能力。</p>
<p>​ 引入Dropout主要是为了防止过拟合。在神经网络中Dropout通过修改神经网络本身结构来实现，对于某一层的神经元，通过定义的概率将神经元置为0，这个神经元就不参与前向和后向传播，就如同在网络中被删除了一样，同时保持输入层与输出层神经元的个数不变，然后按照神经网络的学习方法进行参数更新。在下一次迭代中，又重新随机删除一些神经元（置为0），直至训练结束。<br>Dropout是AlexNet中一个很大的创新，也现在神经网络中的必备结构之一。Dropout也可以看成是一种模型组合，每次生成的网络结构都不一样，通过组合多个模型的方式能够有效地减少过拟合，Dropout只需要两倍的训练时间即可实现模型组合（类似取平均）的效果，非常高效。</p>
<p>二、VGG网络</p>
<p><img src="/2022/08/20/AlexNet%E3%80%81VGG%E3%80%81GoogLeNet-%E6%80%BB%E7%BB%93/3.png"><br>VGG16相比AleNet的一个改进采用连续的几个3<em>3的卷积核代替AlexNet中的较大卷积核（11</em>11,7<em>7,5</em>5）。对于给定的感受野，采用堆积的小卷积核是优于采用大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，而且参数更少。</p>
<p>主要亮点：</p>
<p>（1）结构简洁。VGG由5层卷积层、3层全连接层、softmax输出层构成，层与层之间使用max-pooling分开，所有隐层的激活单元都采用ReLU函数。<br>（2）小卷积核和多卷积子层。VGG使用多个较小卷积核（3x3）的卷积层代替一个卷积核较大的卷积层，一方面可以减少参数，另一方面相当于进行了更多的非线性映射，可以增加网络的拟合&#x2F;表达能力。VGG通过降低卷积核的大小（3x3），增加卷积子层数来达到同样的性能。<br>（3）小池化核。相比AlexNet的3x3的池化核，VGG全部采用2x2的池化核。<br>（4）通道数多。VGG网络第一层的通道数为64，后面每层都进行了翻倍，最多到512个通道，通道数的增加，使得更多的信息可以被提取出来。<br>（5）层数更深、特征图更宽。使用连续的小卷积核代替大的卷积核，网络的深度更深，并且对边缘进行填充，卷积的过程并不会降低图像尺寸。<br>（6）全连接转卷积（测试阶段）。在网络测试阶段将训练阶段的三个全连接替换为三个卷积，使得测试得到的全卷积网络因为没有全连接的限制，因而可以接收任意宽或高为的输入。</p>
<p>细节一：两个3x3的卷积堆叠获得的感受野大小，相当一个5x5的卷积；而3个3x3卷积的堆叠获取到的感受野相当于一个7x7的卷积。<br><img src="/2022/08/20/AlexNet%E3%80%81VGG%E3%80%81GoogLeNet-%E6%80%BB%E7%BB%93/4.png"></p>
<p>好处：（1）可以增加非线性映射；（2）很好地减少参数。</p>
<p>细节二：在网络测试阶段将训练阶段的三个全连接替换为三个卷积，使得测试得到的全卷积网络因为没有全连接的限制。</p>
<p><img src="/2022/08/20/AlexNet%E3%80%81VGG%E3%80%81GoogLeNet-%E6%80%BB%E7%BB%93/5.png"><br>为什么这样做?</p>
<p>原因：输入图像是224x224x3，如果后面三个层都是全连接，那么在测试阶段就只能将测试的图像全部都要缩放大小到224x224x3，才能符合后面全连接层的输入数量要求，这样就不便于测试工作的开展。</p>
<p>举例说明：<br><img src="/2022/08/20/AlexNet%E3%80%81VGG%E3%80%81GoogLeNet-%E6%80%BB%E7%BB%93/6.png"><br>例如7x7x512的层要跟4096个神经元的层做全连接，则替换为对7x7x512的层作通道数为4096、卷积核为1x1的卷积。<br>这个“全连接转卷积”的思路是VGG作者参考了OverFeat的工作思路，例如下图是OverFeat将全连接换成卷积后，则可以来处理任意分辨率（在整张图）上计算卷积，这就是无需对原图做重新缩放处理的优势。</p>
<p>三、GoogLeNet网络</p>
<p>更大的网络容易产生过拟合且计算复杂度太高。针对这两点，GoogLeNet认为最基本的方法是使用稀疏连接代替全连接和卷积操作。</p>
<p>基于保持神经网络结构的稀疏性，又能充分利用密集矩阵的高计算性能的出发点，GoogleNet提出了名为Inception的模块化结构来实现此目的。</p>
<p>Inception是一种网中网（Network In Network）的结构，基于此结构的整个网络的宽度和深度都可扩大，并且能够带来2-3倍的性能提升，Inception目前有v1到v4总共4个版本。<br><img src="/2022/08/20/AlexNet%E3%80%81VGG%E3%80%81GoogLeNet-%E6%80%BB%E7%BB%93/7.png"><br><img src="/2022/08/20/AlexNet%E3%80%81VGG%E3%80%81GoogLeNet-%E6%80%BB%E7%BB%93/8.png"><br>（1） 1x1conv branch<br>    1x1conv branch就是上图中最左侧的分支，利用1x1卷积将网络加宽后进行BatchNorm最后再激活</p>
<p>（2）1x1conv -&gt; 3x3conv branch<br>    这一步卷积核是3x3的尺寸，但是在进行3x3卷积之前，特征图会先经过1x1的卷积层降参（1x1卷积会使网络参数显著降低）</p>
<p>（3）1x1conv -&gt; 5x5conv branch<br>    先经过1x1的卷积降参，后经过5x5的卷积层进行特征提取<br>    InceptionV1中是用的kernel&#x3D;5的卷积核进行特征提取的，在V2中将5x5换成了2个3x3的卷积核，因为二者等效且3x3的卷积参数量约是5x5的卷积操作的1&#x2F;3。所以代码中的2个3x3卷积操作实际上就是图中右侧5x5的卷积操作。</p>
<p>（4）3x3pooling -&gt; 1x1conv<br>    这里虽然用了池化核，但是具体的操作更像卷积。<br>    以往的池化操作步长stride是与卷积核kernel大小相同的，并且不进行填充，这样HxW的特征图经过池化层后大小就变成H&#x2F;s x W&#x2F;s；<br>    这里的池化操作stride&#x3D;1，padding&#x3D;1，kernel&#x3D;3，实际上经过池化操作后特征图大小并不会改变，仅仅是利用池化层来提取与卷积操作不同的特征表达。</p>
<p>要点：<br>1、1x1卷积</p>
<p>可以看到图中有多个黄色的1x1卷积模块，这样的卷积有什么用处呢？</p>
<p>作用①：在相同尺寸的感受野中叠加更多的卷积，能提取到更丰富的特征。这个观点来自于Network in Network，下图里三个1x1卷积都起到了该作用。</p>
<p><img src="/2022/08/20/AlexNet%E3%80%81VGG%E3%80%81GoogLeNet-%E6%80%BB%E7%BB%93/9.png"></p>
<p>作用②：使用1x1卷积进行降维，降低了计算复杂度。</p>
<p><img src="/2022/08/20/AlexNet%E3%80%81VGG%E3%80%81GoogLeNet-%E6%80%BB%E7%BB%93/10.png"></p>
<p>2、辅助分类器<br>    GoogLeNet用到了辅助分类器。GoogleNet一共有22层，除了最后一层的输出结果，中间节点的分类效果也有可能是很好的（例如叶片病虫害分类任务更注重浅层的纹理特征），所以GoogLeNet将中间某一层的输出作为分类，并以一个较小的权重（0.3和0.3）加到最终的分类结果中，一共有2个这样的辅助分类节点。</p>
<p>辅助分类器相当于对模型做了融合，同时给网络增加了反向传播的梯度信号，在一定程度上提供了正则化的作用。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://bynotforget.github.io.git/2022/08/20/AlexNet%E3%80%81VGG%E3%80%81GoogLeNet-%E6%80%BB%E7%BB%93/" data-id="cl7jvhaez00006sfn6e7daz0l" data-title="AlexNet、VGG、GoogLeNet 总结" class="article-share-link">Teilen</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>

    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">深度学习</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">September 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/08/">August 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/09/14/FPN-R-CNN/">FPN &amp; R-CNN</a>
          </li>
        
          <li>
            <a href="/2022/09/01/ShuffleNet-EfficientNet/">ShuffleNet&amp;EfficientNet</a>
          </li>
        
          <li>
            <a href="/2022/08/28/SENet-MobileNet/">SENet&amp;MobileNet</a>
          </li>
        
          <li>
            <a href="/2022/08/25/ResNet%E3%80%81DenseNet/">ResNet、DenseNet</a>
          </li>
        
          <li>
            <a href="/2022/08/20/AlexNet%E3%80%81VGG%E3%80%81GoogLeNet-%E6%80%BB%E7%BB%93/">AlexNet、VGG、GoogLeNet 总结</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 BY<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>