<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>ResNet、DenseNet | A blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="ResNet—什么是残差网络？ 假设我们有如下的一个网络，它可以在训练集和测试集上可以得到很好的性能。接着构造如下的网络，前面4层的参数复制于上面的网络，训练时这几层的参数保持不变。换言之，我们只是在上面的网络新增加了几个紫颜色表示的层。理论上，这个新的网络在训练集或者测试集上的性能要比第一个网络的性能好，毕竟多了几个新增加的层提取特征。然后，实际上这个新的网络却比原先的网络的性能要差。56层的网">
<meta property="og:type" content="article">
<meta property="og:title" content="ResNet、DenseNet">
<meta property="og:url" content="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/index.html">
<meta property="og:site_name" content="A blog">
<meta property="og:description" content="ResNet—什么是残差网络？ 假设我们有如下的一个网络，它可以在训练集和测试集上可以得到很好的性能。接着构造如下的网络，前面4层的参数复制于上面的网络，训练时这几层的参数保持不变。换言之，我们只是在上面的网络新增加了几个紫颜色表示的层。理论上，这个新的网络在训练集或者测试集上的性能要比第一个网络的性能好，毕竟多了几个新增加的层提取特征。然后，实际上这个新的网络却比原先的网络的性能要差。56层的网">
<meta property="og:locale">
<meta property="og:image" content="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/1.png">
<meta property="og:image" content="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/2.png">
<meta property="og:image" content="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/3.png">
<meta property="og:image" content="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/4.png">
<meta property="og:image" content="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/5.png">
<meta property="og:image" content="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/6.png">
<meta property="og:image" content="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/7.png">
<meta property="og:image" content="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/8.png">
<meta property="og:image" content="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/9.png">
<meta property="og:image" content="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/10.png">
<meta property="og:image" content="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/11.png">
<meta property="og:image" content="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/12.png">
<meta property="og:image" content="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/13.png">
<meta property="og:image" content="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/14.png">
<meta property="og:image" content="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/15.png">
<meta property="article:published_time" content="2022-08-25T12:56:45.000Z">
<meta property="article:modified_time" content="2022-08-26T13:39:10.688Z">
<meta property="article:author" content="BY">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/1.png">
  
    <link rel="alternate" href="/atom.xml" title="A blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">A blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://bynotforget.github.io.git"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-ResNet、DenseNet" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/08/25/ResNet%E3%80%81DenseNet/" class="article-date">
  <time class="dt-published" datetime="2022-08-25T12:56:45.000Z" itemprop="datePublished">2022-08-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      ResNet、DenseNet
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>ResNet—什么是残差网络？</p>
<p>假设我们有如下的一个网络，它可以在训练集和测试集上可以得到很好的性能。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/1.png"><br>接着构造如下的网络，前面4层的参数复制于上面的网络，训练时这几层的参数保持不变。换言之，我们只是在上面的网络新增加了几个紫颜色表示的层。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/2.png"><br>理论上，这个新的网络在训练集或者测试集上的性能要比第一个网络的性能好，毕竟多了几个新增加的层提取特征。然后，实际上这个新的网络却比原先的网络的性能要差。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/3.png"><br>56层的网络无论是在训练集还是测试集上，误差率都比20层的要高。出现这种现象的原因并非是由于层数加深引发的梯度消失&#x2F;梯度爆炸，因为已经通过归一化的方法解决了这个问题，对于出现这种现象的原因将在下面讨论，我们将这种反常的现象称之为“退化现象“。</p>
<p>为什么会出现这样的原因呢，何凯明在文章中给出的解释是“难以对网络进行优化”。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/4.png"><br>而退化现象也表明了，实际上新增加的几个紫色的层，很难做到恒等映射。又或者能做到，但在有限的时间内很难完成（即网络要用指数级别的时间才能达到收敛）。</p>
<p>这时候，巧妙的通过添加”桥梁“，使得难以优化的问题瞬间迎刃而解。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/5.png"><br>可以看到通过添加这个桥梁，把数据原封不动得送到FC层的前面，而对于中间的紫色层，可以很容易的通过把这些层的参数逼近于0，进而实现f(x)&#x3D;x的功能。<br>通过跳连接，可以把前四层的输出先送到FC层前面，也就相当于告诉紫色层：“我已经做完前面的工作了，你看看能不能在剩下的工作中发点力，要是找不出提升性能的效果也没事的，我们可以把你的参数逼近于0。”<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/6.png"><br>神经网络无非是拟合一个复杂的函数映射关系，而通过跳链接，可以很好的“切割”这种映射关系，实现“分步”完成。我们把整个映射看成100%，则前面四层网络实现了98%的映射关系，而残余的映射由紫色层完成，Residual 另一个翻译就是”残余，残留“的意思，也就是让每一个残差块，只关注残余映射的一小部分，真的是恰到好处。当然了，实际上网络运行的时候，我们并不会知道哪几层就能达到很好的效果，然后在它们的后面接一个跳连接，于是一开始便在两个层或者三个层之间添加跳连接，形成残差块，每个残差块只关注当前的残余映射，而不会关注前面已经实现的底层映射。大概形状如下图：<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/7.png"></p>
<p>DenseNet</p>
<p>DenseNet的基本思路与ResNet一致，但是它建立的是前面所有层与后面层的密集连接（dense connection），它的名称也是由此而来。DenseNet的另一大特色是通过特征在channel上的连接来实现特征重用（feature reuse）。这些特点让DenseNet在参数和计算成本更少的情形下实现比ResNet更优的性能。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/8.png"><br>相比ResNet，DenseNet提出了一个更激进的密集连接机制：即互相连接所有的层，具体来说就是每个层都会接受其前面所有层作为其额外的输入。图1为ResNet网络的连接机制，作为对比，图2为DenseNet的密集连接机制。可以看到，ResNet是每个层与前面的某层（一般是2~3层）短路连接在一起，连接方式是通过元素级相加。而在DenseNet中，每个层都会与前面所有层在channel维度上连接（concat）在一起（这里各个层的特征图大小是相同的，后面会有说明），并作为下一层的输入。对于一个 L  层的网络，DenseNet共包含 L ( L + 1 )&#x2F; 2 个连接，相比ResNet，这是一种密集连接。而且DenseNet是直接concat来自不同层的特征图，这可以实现特征重用，提升效率，这一特点是DenseNet与ResNet最主要的区别。</p>
<p><img src="/2022/08/25/ResNet%E3%80%81DenseNet/9.png"></p>
<pre><code>ResNet网络的短路连接机制（其中+代表的是元素级相加操作）
</code></pre>
<p><img src="/2022/08/25/ResNet%E3%80%81DenseNet/10.png"></p>
<pre><code>DenseNet网络的密集连接机制（其中c代表的是channel级连接操作）
</code></pre>
<p>DenseNet的前向过程如图所示，可以更直观地理解其密集连接方式，比h3的输入不仅来自h2的x2，还包括前面两层的x1和x2，它们是在channel维度上连接在一起的。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/11.png"><br>DenseNet的密集连接方式需要特征图大小保持一致。为了解决这个问题，DenseNet网络中使用DenseBlock+Transition的结构，其中DenseBlock是包含很多层的模块，每个层的特征图大小相同，层与层之间采用密集连接方式。而Transition模块是连接两个相邻的DenseBlock，并且通过Pooling使特征图大小降低。下图给出了DenseNet的网路结构，它共包含4个DenseBlock，各个DenseBlock之间通过Transition连接在一起。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/12.png"></p>
<p>DenseNet网络结构<br>如前所示，DenseNet的网络结构主要由DenseBlock和Transition组成，如图所示。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/13.png"><br>在DenseBlock中，各个层的特征图大小一致，可以在channel维度上连接。DenseBlock中的非线性组合函数H(.)采用的是BN+ReLU+3x3 Conv的结构，如图所示。另外值得注意的一点是，与ResNet不同，所有DenseBlock中各个层卷积之后均输出k个特征图，即得到的特征图的channel数为k，或者说采用k个卷积核。k在DenseNet称为growth rate，这是一个超参数。一般情况下使用较小的 k（比如12），就可以得到较佳的性能。假定输入层的特征图的channel数为 k0 ，那么 l 层输入的channel数为 k0 + k(l-1)，因此随着层数增加，尽管k设定得较小，DenseBlock的输入会非常多，不过这是由于特征重用所造成的，每个层仅有k个特征是自己独有的。<br><img src="/2022/08/25/ResNet%E3%80%81DenseNet/14.png"></p>
<pre><code>DenseBlock中的非线性转换结构
</code></pre>
<p>由于后面层的输入会非常大，DenseBlock内部可以采用bottleneck层来减少计算量，主要是原有的结构中增加1x1 Conv，如图所示，即BN+ReLU+1x1 Conv+BN+ReLU+3x3 Conv，称为DenseNet-B结构。其中1x1 Conv得到4k个特征图它起到的作用是降低特征数量，从而提升计算效率。</p>
<p><img src="/2022/08/25/ResNet%E3%80%81DenseNet/15.png"></p>
<pre><code>使用bottleneck层的DenseBlock结构 
</code></pre>
<p>对于Transition层，它主要是连接两个相邻的DenseBlock，并且降低特征图大小。Transition层包括一个1x1的卷积和2x2的AvgPooling，结构为BN+ReLU+1x1 Conv+2x2 AvgPooling。另外，Transition层可以起到压缩模型的作用。假定Transition的上接DenseBlock得到的特征图channels数为m ，Transition层可以产生θm个特征，其中θ∈(0,1] 是压缩系数（compression rate）。当θ&#x3D;1 时，特征个数经过Transition层没有变化，即无压缩，而当压缩系数小于1时，这种结构称为DenseNet-C，文中使用θ&#x3D;0.5。对于使用bottleneck层的DenseBlock结构和压缩系数小于1的Transition组合结构称为DenseNet-BC。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://bynotforget.github.io.git/2022/08/25/ResNet%E3%80%81DenseNet/" data-id="cl7jvhafh00036sfnhmoo3h1w" data-title="ResNet、DenseNet" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/08/28/SENet-MobileNet/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          SENet&amp;MobileNet
        
      </div>
    </a>
  
  
    <a href="/2022/08/20/AlexNet%E3%80%81VGG%E3%80%81GoogLeNet-%E6%80%BB%E7%BB%93/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">AlexNet、VGG、GoogLeNet 总结</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">深度学习</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/08/">August 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/08/28/SENet-MobileNet/">SENet&amp;MobileNet</a>
          </li>
        
          <li>
            <a href="/2022/08/25/ResNet%E3%80%81DenseNet/">ResNet、DenseNet</a>
          </li>
        
          <li>
            <a href="/2022/08/20/AlexNet%E3%80%81VGG%E3%80%81GoogLeNet-%E6%80%BB%E7%BB%93/">AlexNet、VGG、GoogLeNet 总结</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 BY<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>